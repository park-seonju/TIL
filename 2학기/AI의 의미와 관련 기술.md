# AI의 의미와 관련 기술

감지하고 있는 것에 반응하여 환경을 인지하고 생각하며 배우고 행동을 취할 수 있는 컴퓨터 시스템

Sense, Think, Learn, Take Action

환경 인식과 행동을 취하는 것이 중요하다. 

------

초기

- 수동적인 업무를 자동화하는 것

현재

- 빠르고 정확한 업무 시스템을 도와주는 역할 -> 전문가 시스템
- 의사결정을 도와주는 역할

미래

- 인간 도움 없이 스스로 결정을 하는 단계

1. 학습 자료
2. 학습 방법,
3. 자동화 관리

어린아이에게 한글을 가르치는 과정

1. 학습자료: 교재

    

   교재를 만들때, 데이터를 정제하는 과정 -> 컴퓨터가 인식할 수 있게끔.

    

   (데이터 정제)

   - 무의미 데이터 제거
   - 형태 변경
   - 학습 자료를 위한 라벨링데이터를 해석하고어떻게 해야하는지?어떤 형태로 만들 것인가?광고나 링크 제거문장 단위 통일고양이, 개, 라벨링문장들 라벨링

2. 학습 방법: 교육법

    

   CNN 을 학습할때, 그거에 맞는 인풋이 필요하다. 어떤 데이터를 사용할 것인가?

    

   데이터를 처음 봤을때, 여기에 가장 적합한 기술은 무엇인가?

    

   어린아이에게는 오감자극이나 호기심, 이야기 등 방법들이 다양하듯이 어떤 기술을 쓸 것인지는 중요한 접근법이다. 

    

   영상 쪽에서는 CNN 을 기반으로 발전을 하였다. 버트 언어 모델에 Classfication 을 붙여서도 사용한다. 

    

   시계열에서는 RNN 등도 쓴다. 

    

   분야마다 적절한 알고리즘등이 있다

3. 자동화/관리: 스스로 공부하는 습관

    

   학습데이터를 항상 만들어줘야 하는데, 이를 자동으로 하는 연구도 진행을 하고 있다. 

    

   데이터 퍼포먼스 유지는 모르기 때문에 항상 추가할 수 있도록 하면 모델 자체에서도 Lifecycle 관리가 필요하다.

인공지능에 대한 기대

사람처럼 행동하도록 만들어진 장치 또는 소프트웨어

- 시간 효율성 - 인간보다 빠른 분석
- 비용 효율성 - 24시간 365 가능
- 객관성 유지 - 항상 같은 기준으로 판단
- 진화 - 인간이 발견하지 못한 특성 이해

가르치는 관점

- 아이가 학습하는 과정을 이해 => 딥러닝 모델 및 성능을 이해하는 단계
- 적은 양의 교재 짧은 학습 시간 => 데이터가 많으면 많을수록
- 학습 효율을 높일 수 있는 방법 => 성능 개선 개념 및 신경망 이론
- 개발한 학습법으로 다른 영역에서도 적용 가능한지 => 드랜스퍼 러닝학습 데이터를 얻기 힘든 분야가 있다.다른 쪽에서 성능 좋은 모델을 가져와서 사용하기도 한다.
- 선생님 개입이 최소화하는 학습 => 인간 개입 최소화
- 학습법을 개발하기 위한 툴 => 이러한 모델을 쉽게 개발할 수 있는
- 스스로 학습하여 발전할 수 있는

------

CNN 기본 구성

- 필터링 이론을 배운다. -> 이미지의 특징을 잡는다.
- CNN 은 영상의 특징을 해석하기 위한 용도
- Pooling - 차원 축소 개념
- Relu 최적화 도움이 안되는 것들을 제거하는
- Fully Connected classfication 특징 값을 재조합하는
- Softmax 확률 값을 내어 다양값을 분류하기 위한

Drop out: 데이터의 오버 피팅을 줄여주는 역할을 한다.

------

CNN 적용 예시

문서 종류 분류 -> VGG16

- 복잡한 특징을 가지면 레이어를 짧게 쌓으면 발견하기 어렵다

기재 및 체크 여부 판단

- 기재 여부는 복잡한 특징이 아니기 때문에 짧게 쌓아도 가능하다.

Bert

MLM Masked Language Model

FINE TUNING

- Sentence Pair Classfication Tasks
- Single Sentence Classfication Tasks
- Question ANswering Tasks
- Single Sentense Tagging Tasks

Multi-layer bidirectional Transformer encoder

BERTbase

L=12 H=768 A=12

BERTlarge

L=24 H=1024 A=16

ex) 나는 학교에 간다.

숫자로 바꿔야 한다. => 임베딩 (의미를 갖고 바꿨다)

- Token embedding: WordPiece embedding (3만 토큰 사전)
- Segment embedding 문장의 앞 뒤를 구분
- position embedding 토큰의 위치 정보를 포함

------

BERT 에서는 Sentence 는 토큰들의 나열

transfomer encoder

Attention (연산)

- 문장 전체 단어들이 전체를 나타내는가?
- 어떻게 - ?

Word2Vec

- 동시 등장 확률

나는 사과를 좋아한다, 나는 사과를 해야한다.

Word2Vec 은 같은 값을 갖는다. 

BERT 에서는 가중치가 다른 값을 나타난다. 